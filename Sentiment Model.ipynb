{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1uQAiST5k4M4C4IQH6FYjtBUkme8xi0i2","authorship_tag":"ABX9TyNAzB6HlApXAy+pCOuP4Q2A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"VwAINWszWbn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749623228865,"user_tz":-600,"elapsed":4188,"user":{"displayName":"Sam Harvey","userId":"11990555985885834709"}},"outputId":"4330bf71-c0a9-4562-d1b3-56eaa1288a83"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["#dependancies\n","import numpy as np\n","import pandas as pd\n","from os import path\n","import matplotlib.pyplot as plt\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pickle\n","import nltk\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["#define Data Pre-Processing\n","def preprocess_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [WordNetLemmatizer().lemmatize(token) for token in tokens if token.isalpha()]\n","    return \" \".join(tokens)\n","\n","with open('/content/drive/MyDrive/Text_Sentiment_ Model/tfidf_vectorizer.pkl', 'rb') as file:\n","    vectorizer = pickle.load(file)\n","\n","#import Data and apply pre-processing\n","df_survey = pd.read_csv('/content/cex_survey_text.csv')\n","df_survey['text'] = df_survey['text'].astype(str).apply(lambda x: x.lower())\n","predict_text = df_survey['text'].astype(str).apply(lambda x: x.lower())\n","predict_text = df_survey['text'].apply(preprocess_text)\n","\n","\n","predict = vectorizer.transform(predict_text)\n","#import and run SVM Model\n","loaded_model = pickle.load(open('/content/drive/MyDrive/Text_Sentiment_ Model/svm_model.pkl', 'rb'))\n","prediction = loaded_model.predict(predict)\n","#attach predictions to data\n","df_survey['label'] = prediction\n","df_survey.to_csv('survey_with_labels.csv', index=False)"],"metadata":{"id":"oMzFHI91W6Bq","executionInfo":{"status":"ok","timestamp":1749623377442,"user_tz":-600,"elapsed":5653,"user":{"displayName":"Sam Harvey","userId":"11990555985885834709"}}},"execution_count":7,"outputs":[]}]}